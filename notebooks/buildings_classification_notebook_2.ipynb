{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildings classification based on facades images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.backend import clear_session\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from building_classification_package.config import CNN_MODEL_CONFIG, \\\n",
    "                                                   VGG_MODEL_CONFIG\n",
    "from building_classification_package.model_utils import train_model, \\\n",
    "                                                        load_trained_and_compiled_model, \\\n",
    "                                                        evaluate_model, \\\n",
    "                                                        model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed in dap to unzip the files which are uploaded (unless I moved them already)\n",
    "\n",
    "# !ls /opt/app-root/s3_home/uploads\n",
    "# !unzip /opt/app-root/s3_home/uploads/buildings_data_smaller.zip -d data\n",
    "\n",
    "\n",
    "#if we are locally, make sure that the data are downloaded in a 'data' folder in the root of the project. \n",
    "#otherwise replace the argument dataset_path below in build_dataset functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build training, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8800 images belonging to 5 classes.\n",
      "Found 2200 images belonging to 5 classes.\n",
      "Found 1358 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from building_classification_package.data_utils import get_data_dir\n",
    "from building_classification_package.data_utils import build_dataset\n",
    "from building_classification_package.config import DATA_CONFIG\n",
    "\n",
    "#here I import datasets preprocessed with the function keras uses to preprocess for vgg model\n",
    "\n",
    "train_iterator = build_dataset(set_to_build='train', \n",
    "                              dataset_path=get_data_dir('train'),\n",
    "                            validation_split=0.2,\n",
    "                              data_config=DATA_CONFIG)\n",
    "\n",
    "val_iterator = build_dataset(set_to_build='val',\n",
    "                                dataset_path = get_data_dir('train'),\n",
    "                                validation_split=0.2,\n",
    "                                data_config=DATA_CONFIG)\n",
    "\n",
    "test_iterator = build_dataset(set_to_build='test',\n",
    "                                dataset_path = get_data_dir('test'),\n",
    "                                data_config=DATA_CONFIG)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not preprocessed sets:\n",
    "\n",
    "DATA_CONFIG_NP = DATA_CONFIG.copy()\n",
    "DATA_CONFIG_NP['preprocessing_fuction'] = None\n",
    "\n",
    "\n",
    "train_iterator_nop = build_dataset(set_to_build='train', \n",
    "                              dataset_path=get_data_dir('train'),\n",
    "                            validation_split=0.2,\n",
    "                              data_config=DATA_CONFIG_NP)\n",
    "\n",
    "val_iterator_nop = build_dataset(set_to_build='val',\n",
    "                                dataset_path = get_data_dir('train'),\n",
    "                                validation_split=0.2,\n",
    "                                data_config=DATA_CONFIG_NP)\n",
    "\n",
    "test_iterator_nop = build_dataset(set_to_build='test',\n",
    "                                dataset_path = get_data_dir('test'),\n",
    "                                data_config=DATA_CONFIG_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets with data augmentation:\n",
    "\n",
    "train_iterator_aug = build_dataset(set_to_build='train', \n",
    "                              dataset_path=get_data_dir('train'),\n",
    "                            validation_split=0.2,\n",
    "                              data_config=DATA_CONFIG, \n",
    "                            shear_range=0.2,zoom_range=0.2,\n",
    "                            rotation_range=70,horizontal_flip=True)\n",
    "\n",
    "val_iterator_aug = build_dataset(set_to_build='val',\n",
    "                                dataset_path = get_data_dir('train'),\n",
    "                                validation_split=0.2,\n",
    "                                data_config=DATA_CONFIG, \n",
    "                                 shear_range=0.2,zoom_range=0.2,\n",
    "                            rotation_range=70,horizontal_flip=True)\n",
    "\n",
    "test_iterator_aug = build_dataset(set_to_build='test',\n",
    "                                dataset_path = get_data_dir('test'),\n",
    "                                data_config=DATA_CONFIG, \n",
    "                                  shear_range=0.2,zoom_range=0.2,\n",
    "                            rotation_range=70,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sizes\n",
    "x_example, y_example = train_iterator[1]\n",
    "\n",
    "input_shape = x_example[0].shape #the input shape should not include batch size\n",
    "n_classes = y_example[0].shape[0]\n",
    "\n",
    "print('size of each image: ', input_shape)\n",
    "print('number of classes: ', n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that the sets work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_iterator[1]\n",
    "X_val, y_val = val_iterator[1]\n",
    "X_test, y_test = test_iterator[1]\n",
    "\n",
    "print ('X_train: ', X_train.shape,\n",
    "       '\\ny_train: ', y_train.shape,\n",
    "       '\\nX_val: ', X_val.shape,\n",
    "       '\\ny_val: ', y_val.shape,\n",
    "       '\\nX_test: ', X_test.shape,\n",
    "       '\\ny_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print an example (both preprocessed and not preprocessed)\n",
    "x,y = train_iterator[0]\n",
    "x_nop, y_nop = train_iterator_nop[0]\n",
    "for i in range(0,1):\n",
    "    image = x[i] \n",
    "    image_nop = x_nop[i]\n",
    "    plt.imshow(image_nop)\n",
    "    plt.show()\n",
    "    print('processed:')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print an example (with and without augmentation)\n",
    "x_aug, y_aug = train_iterator_aug[0]\n",
    "for i in range(0,1):\n",
    "    image = x[i] \n",
    "    image_aug = x_aug[i]\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print('augmented:')\n",
    "    plt.imshow(image_aug)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model\n",
    "\n",
    "The first model I try is a simple CNN model with 2 convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_data=train_iterator,\n",
    "                  val_data=val_iterator,\n",
    "                 model_config=CNN_MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and accuracy:\n",
    "print('validation set:')\n",
    "print(evaluate_model(val_data = val_iterator, model_config=CNN_MODEL_CONFIG))\n",
    "\n",
    "#accuracy on test\n",
    "print ('test set:')\n",
    "evaluate_model(val_data = test_iterator, model_config=CNN_MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predictions = model_predict(CNN_MODEL_CONFIG, val_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with VGG16\n",
    "\n",
    "The second model I try is based on pre-trained VGG16 as base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from building_classification_package.config import VGG_MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_iterator, val_iterator, VGG_MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and accuracy\n",
    "print('validation set:')\n",
    "print(evaluate_model(val_data = val_iterator, model_config=VGG_MODEL_CONFIG))\n",
    "\n",
    "#on unseen test set\n",
    "print('test set:')\n",
    "evaluate_model(val_data = test_iterator, model_config=VGG_MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_predictions = model_predict(VGG_MODEL_CONFIG, val_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check mislabeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = val_iterator.class_indices\n",
    "labels_flip = {value:key for key, value in labels.items()}\n",
    "\n",
    "labels_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real y values of validation set\n",
    "real_y = val_iterator.labels\n",
    "\n",
    "#extract some images and their labels\n",
    "val_iterator.reset()\n",
    "val_sample = [next(val_iterator_nop) for _ in range(150)]\n",
    "x_val_no_p = [x[0] for x in val_sample]\n",
    "\n",
    "#prediction on validation set\n",
    "pred_proba = vgg_predictions\n",
    "pred = [np.argmax(x) for x in pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some examples of correctly labeled images\n",
    "\n",
    "for p, image, y, probs in zip(pred, x_val_nop, real_y, pred_proba):\n",
    "    if p == y:\n",
    "        print(f'{labels_flip[y]} predicted as {labels_flip[p]}')\n",
    "        print(f'probabilities: {labels_flip[0]} {np.round(probs[0], 3)}, \\\n",
    "                               \\n {labels_flip[1]} {np.round(probs[1], 3)}, \\\n",
    "                               \\n {labels_flip[2]} {np.round(probs[2], 3)}, \\\n",
    "                               \\n {labels_flip[3]} {np.round(probs[3], 3)}, \\\n",
    "                               \\n {labels_flip[4]} {np.round(probs[4], 3)}')\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some examples of mislabeled images\n",
    "\n",
    "for p, image, y, probs in zip(pred, x_val_nop, real_y, pred_proba):\n",
    "    if p != y:\n",
    "        print(f'{labels_flip[y]} predicted as {labels_flip[p]}')\n",
    "        print(f'probabilities: {labels_flip[0]} {np.round(probs[0], 3)}, \\\n",
    "                               \\n {labels_flip[1]} {np.round(probs[1], 3)}, \\\n",
    "                               \\n {labels_flip[2]} {np.round(probs[2], 3)}, \\\n",
    "                               \\n {labels_flip[3]} {np.round(probs[3], 3)}, \\\n",
    "                               \\n {labels_flip[4]} {np.round(probs[4], 3)}')\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(real_y, pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(real_y, pred))\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('real class')\n",
    "plt.xticks(ticks=[x + 0.5 for x in list(labels_flip.keys())], labels=list(labels_flip.values()), rotation=90);\n",
    "plt.yticks(ticks=[x + 0.5 for x in list(labels_flip.keys())], labels=list(labels_flip.values()), rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes on the results\n",
    "\n",
    "- industrial is the easiest one to classify correctly, followed by house. \n",
    "\n",
    "- retail and apartment are hardest to classify corretly (also by hand, I noticed, as they are the most ambiguous).\n",
    "- all classes are often classified mistakenly as industrial (it seems like the net has a tendency to classify a lot of things as industrial, hence maybe the high correctly classified industrial images).\n",
    "- On the contrary, very few images are classified as retail and apartments, the hardest to spot.\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
