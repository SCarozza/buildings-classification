{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from keras.backend import clear_session\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Average\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import Callback\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras import Model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import decode_predictions\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.mobilenet import preprocess_input\n",
    "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_input_inc\n",
    "from keras import optimizers\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/app-root/s3_home/uploads\n",
    "# !unzip s3_home/uploads/buildings_data_smallest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the complete dataset has 8 classes.\n",
    "#the 'smaller' version 5 classes\n",
    "#the 'smallest' version 3 classes\n",
    "\n",
    "\n",
    "which_data = '_smaller' #choice between '', '_smaller', '_smallest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessed for vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to keep as many obs as possible in train, so I split the test in test and validation\n",
    "\n",
    "#define generators\n",
    "datagen_vgg_train = ImageDataGenerator(featurewise_center = False,\n",
    "#                             rescale = 1/256,\n",
    "                            preprocessing_function=preprocess_input_vgg)\n",
    "datagen_vgg_val_test = ImageDataGenerator(featurewise_center = False,\n",
    "#                             rescale = 1/256,\n",
    "                            preprocessing_function=preprocess_input_vgg,\n",
    "                            validation_split=0.4)\n",
    "\n",
    "#version not processed\n",
    "datagen_vgg_train_nop = ImageDataGenerator(rescale=1/256)\n",
    "datagen_vgg_val_test_nop = ImageDataGenerator(rescale=1/256, validation_split=0.4)\n",
    "\n",
    "\n",
    "#import data\n",
    "train_in_vgg = datagen_vgg_train.flow_from_directory('Building_labeled_train_data'+str(which_data),\n",
    "#                                       color_mode=\"rgb\",\n",
    "#                                        target_size = (128, 128),\n",
    "#                                        batch_size=32,\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       shuffle=True,\n",
    "                                       seed=42)\n",
    "train_in_vgg_nop = datagen_vgg_train_nop.flow_from_directory('Building_labeled_train_data'+str(which_data), seed=42, shuffle=True)\n",
    "\n",
    "val_in_vgg = datagen_vgg_val_test.flow_from_directory('Building_labeled_test_data'+str(which_data),\n",
    "#                                       color_mode=\"rgb\",\n",
    "#                                        target_size = (128, 128),\n",
    "#                                        batch_size=32,\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       shuffle=True,\n",
    "                                       seed=42,\n",
    "                                       subset='training') #this is because I used train_test split in the generator for val and test\n",
    "val_in_vgg_nop = datagen_vgg_val_test_nop.flow_from_directory('Building_labeled_test_data'+str(which_data), seed=42, subset='training', shuffle=True)\n",
    "\n",
    "test_in_vgg = datagen_vgg_val_test.flow_from_directory('Building_labeled_test_data'+str(which_data),\n",
    "#                                       color_mode=\"rgb\",\n",
    "#                                       target_size = (128, 128),\n",
    "#                                        batch_size=32,\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       shuffle=True,\n",
    "                                       seed=42,\n",
    "                                       subset='validation') #this is because I used train_test split in the generator for val and test\n",
    "test_in_vgg_nop = datagen_vgg_val_test_nop.flow_from_directory('Building_labeled_test_data'+str(which_data), seed=42, subset='validation', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_in_vgg.next()\n",
    "X_val, y_val = val_in_vgg.next()\n",
    "X_test, y_test = test_in_vgg.next()\n",
    "\n",
    "X_train_nop, y_train_nop = train_in_vgg_nop.next()\n",
    "X_val_nop, y_val_nop = val_in_vgg_nop.next()\n",
    "X_test_nop, y_test_nop = test_in_vgg_nop.next()\n",
    "\n",
    "print ('X_train: ', X_train.shape,\n",
    "       '\\ny_train: ', y_train.shape,\n",
    "       '\\nX_val: ', X_val.shape,\n",
    "       '\\ny_val: ', y_val.shape,\n",
    "       '\\nX_test: ', X_test.shape,\n",
    "       '\\ny_test: ', y_test.shape)\n",
    "\n",
    "#print an example\n",
    "x,y = val_in_vgg.next()\n",
    "x_nop, y_nop = val_in_vgg_nop.next()\n",
    "for i in range(0,1):\n",
    "    image = x[i] \n",
    "    image_nop = x_nop[i]\n",
    "    plt.imshow(image_nop)\n",
    "    plt.show()\n",
    "    print('processed:')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape #the input shape should not include batch size\n",
    "n_classes = y_train[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_vgg = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=70,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=preprocess_input_vgg)\n",
    "\n",
    "val_test_datagen_vgg = ImageDataGenerator(preprocessing_function=preprocess_input_vgg,\n",
    "                                        validation_split = 0.4)\n",
    "\n",
    "\n",
    "#import data\n",
    "train_in_vgg_aug = train_datagen_vgg.flow_from_directory('Building_labeled_train_data'+str(which_data),\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       shuffle=True,\n",
    "                                       seed=42)\n",
    "\n",
    "val_in_vgg_aug = val_test_datagen_vgg.flow_from_directory('Building_labeled_test_data'+str(which_data),\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       shuffle=False,\n",
    "                                       seed=42,\n",
    "                                       subset='training')\n",
    "\n",
    "test_in_vgg_aug = val_test_datagen_vgg.flow_from_directory('Building_labeled_test_data'+str(which_data),\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       shuffle=False,\n",
    "                                       seed=42,\n",
    "                                       subset='validation')\n",
    "\n",
    "\n",
    "# train_in_inc_aug = train_datagen_inc.flow_from_directory('Building_labeled_train_data'+str(which_data),\n",
    "#                                        class_mode=\"categorical\",\n",
    "#                                        shuffle=True,\n",
    "#                                        seed=42)\n",
    "\n",
    "# test_in_inc_aug = test_datagen_inc.flow_from_directory('Building_labeled_test_data'+str(which_data),\n",
    "#                                        class_mode=\"categorical\",\n",
    "#                                        shuffle=True,\n",
    "#                                        seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbakcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearning(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(20,5))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.set_yscale('log')\n",
    "        ax1.plot(self.x, self.loss, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_loss, label=\"val_loss\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(self.x, self.accuracy, label=\"accuracy\")\n",
    "        ax2.plot(self.x, self.val_accuracy, label=\"validation accuracy\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_learning = PlotLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0.005, \n",
    "                           patience=15, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducelr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cnn_model(input_shape):\n",
    "    X_input = Input((input_shape))\n",
    "    \n",
    "    #conv\n",
    "    X = Conv2D(16, (3,3), name='conv0')(X_input)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2), name='max_pool0')(X)\n",
    "    \n",
    "    X = Conv2D(25, (6,6), name='conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2), name='max_pool1')(X)\n",
    "\n",
    "    #dropout\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    #rest\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    #add one dense layer\n",
    "    X = Dense(150, activation='relu', name='dense')(X)\n",
    "    \n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    #final prediction\n",
    "    X = Dense(n_classes, activation='softmax', name='final_dense')(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X,  name='basic_cnn')\n",
    "    #here we are only building the model, that starting from X_input leads to (the last\n",
    "    #X) through all the layers\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = basic_cnn_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(learning_rate=0.001, decay=0.001)\n",
    "\n",
    "sgd2 = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer=sgd, loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit_generator(train_in_vgg, epochs=50, \n",
    "                                  validation_data=val_in_vgg, \n",
    "                                  callbacks=[early_stop, plot_learning, reducelr],\n",
    "                                  use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('initial accuracy on train: ', cnn_model.history.history['accuracy'][0])\n",
    "print('final accuracy on train: ', cnn_model.history.history['accuracy'][-1])\n",
    "\n",
    "print('initial accuracy on val: ', cnn_model.history.history['val_accuracy'][0])\n",
    "print('final accuracy on val: ', cnn_model.history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('s3_home/uploads/base_model_vgg128.pickle', 'rb') as f:\n",
    "#     base_model_vgg128 = pickle.load(f)\n",
    "    \n",
    "with open('/opt/app-root/s3_home/uploads/base_model_vgg256.pickle', 'rb') as f:\n",
    "    base_model_vgg256 = pickle.load(f)\n",
    "    \n",
    "with open('/opt/app-root/s3_home/uploads/base_model_inceptionv3_256.pickle', 'rb') as f:\n",
    "    base_model_inception256 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = base_model_vgg256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the end of the net\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "# # x = Dropout(0.2)(x)\n",
    "# # x = Dense(256, activation='relu')(x)\n",
    "# # x = Dropout(0.3)(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "# x = Dropout(0.4)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(n_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I make 2 models here because I want to try different lr and then ensemble them\n",
    "model_vgg1 = Model(inputs=base_model.input, outputs=predictions)\n",
    "# model_vgg2 = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the base model layers\n",
    "for layer in base_model.layers[:18]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in base_model.layers[18:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_vgg1.layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline accuracy:\n",
    "1/n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(learning_rate=0.001, decay=0.001)\n",
    "\n",
    "sgd2 = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "\n",
    "#used sgd1 till 40 epochs, went up to 0.7-08 for tr and 0.5-0.6 for val\n",
    "\n",
    "model_vgg1.compile(optimizer=sgd2, loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "vgg1_history = model_vgg1.fit_generator(train_in_vgg, \n",
    "                                         epochs = 20,\n",
    "                                         validation_data=val_in_vgg, \n",
    "                                         callbacks=[early_stop, plot_learning, reducelr],\n",
    "                                         validation_freq=1,\n",
    "                                         use_multiprocessing=True\n",
    "                                         )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mislabeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#prediction on validation set\n",
    "pred_proba = model_vgg1.predict_generator(val_in_vgg)\n",
    "pred = [np.argmax(x) for x in pred_proba]\n",
    "\n",
    "#real values\n",
    "real_y_proba = y_val\n",
    "real_y = [np.argmax(x) for x in real_y_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correctly labeled\n",
    "labels = {0:'apt', 1:'office'}\n",
    "\n",
    "for p, image, y, probs in zip(pred, X_val_nop, real_y, pred_proba):\n",
    "    if p == y:\n",
    "        print(f'{labels[y]} predicted as {labels[p]}')\n",
    "        print(f'probabilities: apt {np.round(probs[0], 3)}, office {np.round(probs[1], 3)}')\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mislabeled\n",
    "labels = {0:'apt', 1:'office'}\n",
    "\n",
    "for p, image, y, probs in zip(pred, X_val_nop, real_y, pred_proba):\n",
    "    if p != y:\n",
    "        print(f'{labels[y]} predicted as {labels[p]}')\n",
    "        print(f'probabilities: apt {np.round(probs[0], 3)}, office {np.round(probs[1], 3)}')\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a model with same architecture but different optimizer... to use for ensemble\n",
    "\n",
    "# %timeit\n",
    "\n",
    "# model_vgg2.compile(optimizer=sgd2, loss='categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# vgg2_history = model_vgg2.fit_generator(train_in_vgg, \n",
    "#                                          epochs=15,\n",
    "#                                          validation_data=val_in_vgg, \n",
    "#                                          callbacks=[early_stop, plot_learning, reducelr],\n",
    "#                                          validation_freq=1,\n",
    "#                                          use_multiprocessing=True\n",
    "#                                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first model\n",
    "\n",
    "print('initial accuracy on train: ', vgg1_history.history['accuracy'][0])\n",
    "print('final accuracy on train: ', vgg1_history.history['accuracy'][-1])\n",
    "\n",
    "print('initial accuracy on val: ', vgg1_history.history['val_accuracy'][0])\n",
    "print('final accuracy on val: ', vgg1_history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second model\n",
    "\n",
    "print('initial accuracy on train: ', vgg2_history.history['accuracy'][0])\n",
    "print('final accuracy on train: ', vgg2_history.history['accuracy'][-1])\n",
    "\n",
    "print('initial accuracy on val: ', vgg2_history.history['val_accuracy'][0])\n",
    "print('final accuracy on val: ', vgg2_history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emseble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test ensemble\n",
    "# from keras.layers import Average\n",
    "\n",
    "# def ensemble(models, model_input):\n",
    "#     outputs = [model.outputs[0] for model in models]\n",
    "#     y = Average()(outputs)\n",
    "#     model = Model(model_input, y, name='ensemble')\n",
    "#     return model\n",
    "\n",
    "# ens_model = ensemble([model_vgg1, model_vgg2], test_in_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities = model.predict_generator(test_in_aug)\n",
    "\n",
    "filenames = test_in_vgg.filenames\n",
    "nb_samples = len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict = model.predict_generator(test_in_aug,steps = nb_samples)\n",
    "# predicted_class_indices=np.argmax(predict,axis=1)\n",
    "# labels = test_in_aug.classes\n",
    "\n",
    "loss1, acc1 = model_vgg1.evaluate_generator(test_in_vgg, steps=nb_samples, verbose=0)\n",
    "loss2, acc2 = model_vgg2.evaluate_generator(test_in_vgg, steps=nb_samples, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1, acc1, loss2, acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    counter = 0\n",
    "    num = List[0] \n",
    "    for i in List: \n",
    "        curr_frequency = List.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            num = i \n",
    "    return num \n",
    "  \n",
    "\n",
    "\n",
    "def calculate_ensemble_pred(model1, model2, test_data):\n",
    "    pred1 = model1.predict_generator(test_data)\n",
    "    pred2 = model2.predict_generator(test_data)\n",
    "    pred1_classes = [np.argmax(x) for x in pred1]\n",
    "    pred2_classes = [np.argmax(x) for x in pred2]\n",
    "    \n",
    "    votes = [(pred1_classes[x], pred2_classes[x]) for x in range(len(pred1))]\n",
    "    final_votes = [most_frequent(x) for x in votes]\n",
    "    return final_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1 = cnn_model.predict_generator(test_in_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = calculate_ensemble_pred(model_vgg1, model_vgg2, test_in_vgg)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(mn_history.history['accuracy'])\n",
    "# plt.plot(mn_history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(mn_history.history['loss'])\n",
    "# plt.plot(mn_history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1: my cnn\n",
    "\n",
    "model1 = cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2: the vgg\n",
    "\n",
    "model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 3: inceptionv3\n",
    "\n",
    "model3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('s3_home/uploads/base_model_inceptionv3_256.pickle', 'rb') as f:\n",
    "    base_model_inception = pickle.load(f)\n",
    "\n",
    "# base_model_inception.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
